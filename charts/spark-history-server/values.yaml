global:
  secrets:
    gcs:
      enable: false
      GCP_SERVICE_ACCOUNT_EMAIL: "XXXX-compute@developer.gserviceaccount.com OR XXXX@XXXX.iam.gserviceaccount.com"
      GCP_DEFAULT_REGION: us-west1
      GCP_DEFAULT_ZONE: us-west1-c
    aws:
      enable: true
      use_iam_role: false # Set this to true and AWS_IAM_ROLE_ARN to appropriate IAM ARN in case of IAM role usage
      AWS_IAM_ROLE_ARN: arn:aws:iam::XXXX:role/XXXX
      AWS_ACCESS_KEY_ID: XXXX
      AWS_SECRET_ACCESS_KEY: XXXX
      AWS_DEFAULT_REGION: us-west-2
    azure:
      enable: false
      STORAGE_ACCOUNT_NAME: XXXX
      STORAGE_ACCOUNT_ACCESS_KEY: XXXX
      SPARK_HS_CONTAINER_NAME: sparkhs

replicaCount: 1
nameOverride: ""
fullnameOverride: ""

rbac:
  create: true

serviceAccount:
  create: true
  name:

image:
  repository: snappyflowml/spark-history-server
  tag: azurestage-1
  pullPolicy: IfNotPresent

service:
  type: NodePort
  nodePort: 32080
  port:
    number: 18080
    name: http-historyport
  annotations: {}

environment:
# Note: do not configure Spark history events directory using SPARK_HISTORY_OPTS. It will be
# configured by this chart based on the values in "pvc", "gcs" or "hdfs" attribute.
  # SPARK_HISTORY_OPTS: ...
  # SPARK_DAEMON_MEMORY: 1g
  # SPARK_DAEMON_JAVA_OPTS: ...
  # SPARK_DAEMON_CLASSPATH: ...
  # SPARK_PUBLIC_DNS: ...

podAnnotations: {}

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  #
  # To let the application start up quickly give it a big limit
  # limits:
  #  cpu: 1000m
  #  memory: 1Gi
  # requests:
  #  cpu: 100m
  #  memory: 512Mi

ingress:
  enabled: false
  annotations: {}
  # kubernetes.io/ingress.class: nginx
  # kubernetes.io/tls-acme: "true"
  path: /
  hosts:
    - spark-history-server.example.com
  tls: []
  #  - secretName:spark-history-server.example.com
  #    hosts:
  #      - spark-history-server.example.com

pvc:
  # to use a file system path for Spark events dir, set 'enablePVC' to true and mention the
  # name of an already created persistent volume claim in existingClaimName.
  # The volume will be mounted on /data in the pod
  enablePVC: false
  existingClaimName: nfs-pvc
  eventsDir: "/"

# Settings for the sub-chart
# When pvc.enablePVC is true, make sure:
# pvc.existingClaimName == nfs.pvcName
nfs:
  enableExampleNFS: false
  pvName: nfs-pv
  pvcName: nfs-pvc

gcs:
  secret: history-secrets
  key: sparkonk8s.json
  logDirectory: gs://spark-hs/

hdfs:
  enableHDFS: false
  hdfsSiteConfigMap: hdfs-site
  coreSiteConfigMap: core-site
  logDirectory: hdfs://hdfs/history/
  HADOOP_CONF_DIR: /etc/hadoop

s3:
  logDirectory: s3a://sparkhs/spark-hs
  # custom s3 endpoint. Keep default for using aws s3 endpoint
  endpoint: default

wasbs:
  enableWASBS: false
  sasKeyMode: false
  logDirectory: wasbs://spark-hs

imagePullSecrets: []

nodeSelector: {}

tolerations: []

affinity: {}
