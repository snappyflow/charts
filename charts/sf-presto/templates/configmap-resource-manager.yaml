{{- $result := mulf (mulf .Values.resourcemanager.podmem 0.8) 1 }}
{{- $resultInt := int $result }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "sf-presto.fullname" . }}-resource-manager
  labels:
    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version }}
    app.kubernetes.io/name: {{ .Chart.Name }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/component: resource-manager
    app.kubernetes.io/version: {{ .Values.image.tag | default .Chart.AppVersion | quote }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
data:
  node.properties: |-
    node.environment=production
    node.data-dir=/var/presto/data
  jvm.config: |-
    -server
    -Xmx{{ $resultInt }}M
    -XX:+UseG1GC
    -XX:G1HeapRegionSize=32M
    -XX:+UseGCOverheadLimit
    -XX:+ExplicitGCInvokesConcurrent
    -XX:+HeapDumpOnOutOfMemoryError
    -XX:+ExitOnOutOfMemoryError
    -Djdk.attach.allowAttachSelf=true
    -Duser.timezone=UTC
  config.properties: |-
    coordinator=false
    http-server.http.port={{ .Values.service.port }}
    discovery.uri=http://{{ .Release.Name }}-discovery:{{ .Values.service.port }}
    discovery-server.enabled=true
    node-scheduler.include-coordinator=false
    resource-manager-enabled=true
    resource-manager=true
    experimental.query-limit-spill-enabled=true
    thrift.server.port=8081
    thrift.server.ssl.enabled=false

    # Refer https://aws.amazon.com/blogs/big-data/top-9-performance-tuning-tips-for-prestodb-on-amazon-emr/ & https://techjogging.com/memory-setup-prestodb-cluster.html
    query.max-queued-queries=5000
    query.max-history=1000
    query.max-concurrent-queries=40
    query.client.timeout=60m
    query.execution-policy=phased
    task.concurrency=4
    task.max-worker-threads=16
    task.http-response-threads=100

    # Cluster level config depends on worker size
    query.max-memory-per-node={{ mulf .Values.worker.podmem 0.32 }}MB
    query.max-total-memory-per-node={{ mulf .Values.worker.podmem 0.384 }}MB
    query.max-memory={{ mulf .Values.worker.podmem 0.32 .Values.autoscaling.worker.maxReplicas }}MB
    query.max-total-memory={{ mulf .Values.worker.podmem 0.384 .Values.autoscaling.worker.maxReplicas }}MB
    memory.heap-headroom-per-node={{ mulf .Values.worker.podmem 0.16 }}MB

  log.properties: |-
    com.facebook.presto=DEBUG
